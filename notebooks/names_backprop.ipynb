{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff59979",
   "metadata": {},
   "source": [
    "# Makemore Back Propagation \n",
    "\n",
    "Implementing back prop from scratch instead of using loss.backward(). \n",
    "\n",
    "Manual back prop through cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba31e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "\n",
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1529da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9558dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c04b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item() # check whether all elements in tensor exactly equal\n",
    "  app = torch.allclose(dt, t.grad)    # check whether all elements in tensor approx equal \n",
    "  maxdiff = (dt - t.grad).abs().max().item() # check max difference \n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb406f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 \n",
    "n_hidden = 64 \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.rand((vocab_size, n_embd),                generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden),   generator=g) * 5/3/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                          generator=g) * 0.1 # use for fun, useless bc of BN \n",
    "W2 = torch.randn((n_hidden, vocab_size),            generator=g) * 0.1 \n",
    "b2 = torch.randn(vocab_size,                        generator=g) * 0.1 \n",
    "bngain = torch.randn((1, n_hidden))* 0.1 + 1.0 \n",
    "bnbias = torch.randn((1, n_hidden))* 0.1\n",
    "# Note: initialising many of these params (bnbias) in non standard ways because initialising with all 0 will mask an incorrect backward pass\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]               \n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters: \n",
    "    p.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86bfafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32 # batch size \n",
    "\n",
    "# construct minibatch \n",
    "ix = torch.randint(0, Xtr.shape[0], (n, ), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf26ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5445, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backprop through \n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "# logits = xenc @ W \n",
    "# counts = logits.exp()\n",
    "# probs = counts / counts.sum(1, keepdim=True)\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability since we are exponentiating later\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4125e834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through all variables manually using definitions in forward pass \n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0/probs) * dlogprobs # d(logx)dx = 1/x\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim = True) \n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = -(counts_sum**-2)* dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts #d(norm_logits.exp()) = norm_logits.exp() = counts \n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T \n",
    "dW2 = h.T @ dlogits \n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh # d(tanh) = 1-tanh**2 \n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim = True )\n",
    "dbnraw = bngain * dhpreact \n",
    "dbnbias = dhpreact.sum(0, keepdim = True)\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim = True)\n",
    "dbndiff = (bnvar_inv * dbnraw)\n",
    "dbnvar = -0.5*(bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1)*torch.ones_like(bndiff2) * dbnvar) \n",
    "dbndiff += 2*bndiff * dbndiff2\n",
    "dbnmeani = (-dbndiff).sum(0, keepdim= True)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += 1/n*torch.ones_like(hprebn) * dbnmeani \n",
    "dembcat = dhprebn @ W1.T \n",
    "dW1 = embcat.T @ dhprebn \n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]): \n",
    "    for j in range(Xb.shape[1]): \n",
    "        ix = Xb[k, j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar) \n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2158278",
   "metadata": {},
   "source": [
    "Note: sum(1) - sum horizontally across columns, collapse columns into 1 col \n",
    "sum(0) - vertically across rows, collapse rows into 1 row \n",
    "\n",
    "dlogprobs: `loss = -logprobs[range(n), Yb].mean()` \n",
    "* logprobs (32, 27), Yb (32)\n",
    "* forward pass: for row 0 in logprobs, pluck out Yb[0]th element (aka the logprob assigned to the correct element for example 0). go down the rows using iterator range(n)\n",
    "* these elements get plucked out and their mean becomes loss \n",
    "* L = (-a-b-c)/n = aa/n -b/n -c/n --> dL/da = -1/n \n",
    "* For the other numbers that do not get plucked out -- dLdx = 0 \n",
    "* dlogprobs should also be (32, 27), each row has 26 0s and 1 non zero in the position that hasbeen plucked out \n",
    "\n",
    "dprobs: `logprobs = probs.log()`\n",
    "* dprobs (32, 27), dlogprobs (32, 27)\n",
    "* d(logx)dx = 1/x \n",
    "* so we get `dprobs = (1.0/probs) * dlogprobs`\n",
    "* intuitively: for correct chars that are assigned a low prob, 1/probs will be big, and will multiply by the dlogprobs. essentially it is boosting the gradient for wrong predictions\n",
    "\n",
    "dcounts_sum_inv: `probs = counts * counts_sum_inv`\n",
    "* counts (32, 27), counts_sum_inv (32, 1) \n",
    "* for counts_sum_inv, broadcasted in forward pass, col vector replicated for 27 cols, each element used 27 times. gradients should sum horizontally .sum(1, keepdim = True)\n",
    "* backprob thru element wise multiplication: dcounts_sum_inv = counts * dprobs  \n",
    "* backprob thru replication: dcounts_sum_inv = (counts * dprobs).sum(1, keepdim = True)\n",
    "\n",
    "dcounts: `counts_sum = counts.sum(1, keepdims=True)`\n",
    "* counts (32, 27), counts_sum (32, 1) \n",
    "* a11 a12 a13 --> b1 (=a11+a12+a13)\n",
    "* a21 a22 a23 --> b2 (=a21+a22+a23)\n",
    "* deriv of b1 wrt a11 a12 a13 are 1, and wrt the rest are 0 \n",
    "* in chain rule, addition is like  arouter of gradients. whatever gradient comes from above gets routed to the the elements that participate in the addition\n",
    "* dcounts = dcounts_sum.sum(1, keepdim= True)\n",
    "\n",
    "dlogits: `norm_logits = logits - logit_maxes`\n",
    "* a11 a12 a13 = b11 b12 b13 - c1 \n",
    "* a21 a22 a23 = b21 b22 b23 - c2\n",
    "* a22 = b22 - c2; db = 1, dc = -1 and since it is reused for all columns need to sum gradients horizontally\n",
    "* note: the only reason why we calculate logit_maxes is so that norm-logits.exp() doesnt overflow later. values in logit_maxes should not affect probs or loss. hence grad wrt them should be 0 or extremely small values. and it is! \n",
    "\n",
    "dlogits: `logit_maxes = logits.max(1, keepdim=True).values`\n",
    "* logits (32, 27) logit_maxes(32, 1).\n",
    "* for each row in logits, the position that contains max has local deriv 1 and others have local deriv 0. then multiply with logit_maxes\n",
    "* use one hot encoding to get 1 at positions of logits.max, num_classes=logits.shape[1]\n",
    "\n",
    "dh, dW2, db2: `logits = h @ W2 + b2`\n",
    "* dlogits (32, 27), dh (32, 64), dW2 (64, 27), b2 (27)  \n",
    "* dX must be same dim as X, and dh is a matrix mul of dW2.T and dh (vice versa). theres only one way to multiply it so the dims work out \n",
    "* db2 is the sum of dlogits in some dim. since the dims must be 1,27 in the end, we are taking sum in 0th dimension. (note in broadcasting b2 became (1, 27) and got replicated for 32 rows, so sum across the rows) \n",
    "\n",
    "dbngain, dbnraw, dbnbias: `hpreact = bngain * bnraw + bnbias`\n",
    "* hpreact (32, 64), bngain (1, 64), bnraw (32, 64), bnbias (1, 64)\n",
    "* in broadcasting, bngain is being replicated into to 32 rows. each element is used 32 times. gradient must sum across rows. \n",
    "* same for bnbias \n",
    "\n",
    "dbndiff2: `bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)`\n",
    "* bnvar (1, 64) bndiff2(32, 64)\n",
    "* a11 a12 a13 --> b11 (= 1/(n-1)*(a11+a12+a13))\n",
    "* dbndiff2 = 1/(n-1)*torch.ones_like(bndiff2) * dbnvar, 32,64 * 1,64 -- broadcasting where 1,64 is copied across 64 rows, after multiplication get 32,64\n",
    "\n",
    "dhprebn: `bnmeani = 1/n*hprebn.sum(0, keepdim=True)`\n",
    "* bnmeani (1,64), hprebn (32,64), sum horizontally \n",
    "* dhprebn = 1/n*torch.ones_like(hprebn) * dbnmeani \n",
    "\n",
    "demb: `embcat = emb.view(emb.shape[0], -1)`\n",
    "* embcat (32, 30) emb (32, 3, 10)\n",
    "* demb = dembcat.view(emb.shape)\n",
    "\n",
    "emb = C[Xb]\n",
    "* emb (32, 3, 10) C(27, 10) Xb(32, 3)\n",
    "* in forward pass we used each value in Xb to index into the right row of C, and deposited into emb at the correct position \n",
    "* undo the indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53504434",
   "metadata": {},
   "source": [
    "### Exercise 2: Backprop through cross entropy in 1 go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff97ae4",
   "metadata": {},
   "source": [
    "Previously we implemented backprop individually through all the forward pass steps, but we can actually derive a simpler expression for loss and from there get the derivative, simplify the expression, and just write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cc9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.544471025466919 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc0f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass after math derivation \n",
    "dlogits = F.softmax(logits, 1)  # apply softmax of rows of logits\n",
    "dlogits[range(n), Yb] -= 1      # subtract 1 from the correct element in each logit\n",
    "dlogits /= n                    # back prop through mean\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "334a2e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x178bf6890>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')\n",
    "# black squares are the positions of the correct index where we subtracted 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c90e55",
   "metadata": {},
   "source": [
    "These are the derivatives of dlogits \n",
    "* for each example (each row), dlogits is equal to probabilities (softmax of logits) for everything except for the correct char, where dlogits = probs-1 \n",
    "* if take dlogits[0].sum(), get 0 \n",
    "* think of dlogits (gradients) as a force that pulls down on probs of incorrect characters and pulls up on probs of correct characters. the amount of push and pull is exactly equalised because they sum to 0.  \n",
    "* amount to which we push and pull is proportional to how far off our probabilities are. if probs were exactly correct (everything is 0 except correct position is 1), then dlogits row has everything as 0, meaning no push and pull force "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dc3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0234, 0.0678, 0.0658, 0.0415, 0.0273, 0.0244, 0.0335, 0.0240, 0.0150,\n",
       "        0.0144, 0.0577, 0.0667, 0.0259, 0.0292, 0.0373, 0.0138, 0.0675, 0.0249,\n",
       "        0.0619, 0.0655, 0.0120, 0.0308, 0.0405, 0.0244, 0.0573, 0.0225, 0.0250],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities of each char for first example \n",
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0234,  0.0678,  0.0658,  0.0415,  0.0273,  0.0244,  0.0335,  0.0240,\n",
       "         0.0150, -0.9856,  0.0577,  0.0667,  0.0259,  0.0292,  0.0373,  0.0138,\n",
       "         0.0675,  0.0249,  0.0619,  0.0655,  0.0120,  0.0308,  0.0405,  0.0244,\n",
       "         0.0573,  0.0225,  0.0250], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dlogits for first example (multiplied by n)\n",
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984d743",
   "metadata": {},
   "source": [
    "### Backprop through batchnorm in 1 go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0b050",
   "metadata": {},
   "source": [
    "Previously we implemented backprop individually through all the forward pass steps, but we can actually derive a simpler expression for the output of batchnorm `hpreact` and take the derivative wrt its input `hprebn`, simplify the expression, and just write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c69fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:  \n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dcdaf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass mathematically derived \n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5a7f30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f5cc4",
   "metadata": {},
   "source": [
    "### Exercise 4: Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40454172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.8129\n",
      "  10000/ 200000: 2.1830\n",
      "  20000/ 200000: 2.3641\n",
      "  30000/ 200000: 2.4971\n",
      "  40000/ 200000: 1.9738\n",
      "  50000/ 200000: 2.3662\n",
      "  60000/ 200000: 2.3811\n",
      "  70000/ 200000: 1.9873\n",
      "  80000/ 200000: 2.4326\n",
      "  90000/ 200000: 2.1775\n",
      " 100000/ 200000: 1.9943\n",
      " 110000/ 200000: 2.3453\n",
      " 120000/ 200000: 2.0327\n",
      " 130000/ 200000: 2.4296\n",
      " 140000/ 200000: 2.3059\n",
      " 150000/ 200000: 2.1114\n",
      " 160000/ 200000: 1.9403\n",
      " 170000/ 200000: 1.8315\n",
      " 180000/ 200000: 1.9940\n",
      " 190000/ 200000: 1.8742\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "n_embd = 10 # dimensionality of character embedding vectors\n",
    "n_hidden = 200 # number of neurons in hidden layer \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # used only for correctness comparison\n",
    "\n",
    "    # manual backprop \n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    #if i >= 100: # to delete early breaking when you're ready to train the full net\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc2881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "(30, 200)       | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n",
      "(200,)          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n",
      "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "(27,)           | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\n"
     ]
    }
   ],
   "source": [
    "# to check: insert early break, uncomment out loss.backward(), comment out with torch.no_grad()\n",
    "# for checking gradients:\n",
    "for p,g in zip(parameters, grads):\n",
    "    cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a77f514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate BatchNorm for interference (get overall mean and var) \n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1d255e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0706355571746826\n",
      "val 2.1094236373901367\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b0a4ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahxqto.\n",
      "hqvifi.\n",
      "mrixreetl.\n",
      "hklansaeja.\n",
      "hnen.\n",
      "qpnrrhc.\n",
      "kaqhi.\n",
      "oremari.\n",
      "cemiiv.\n",
      "kklegg.\n",
      "hhlm.\n",
      "eoi.\n",
      "dqshnn.\n",
      "shlin.\n",
      "ariadbq.\n",
      "wane.\n",
      "ogdiarixi.\n",
      "fkcekphrran.\n",
      "ea.\n",
      "ecoia.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c162b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahzamille.\n",
      "khi.\n",
      "mri.\n",
      "reety.\n",
      "skanden.\n",
      "jazonel.\n",
      "deliah.\n",
      "jareei.\n",
      "nellara.\n",
      "chaily.\n",
      "kaleigh.\n",
      "ham.\n",
      "joce.\n",
      "quinthorline.\n",
      "liven.\n",
      "corterri.\n",
      "jarisi.\n",
      "jaxen.\n",
      "dustine.\n",
      "deciia.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
